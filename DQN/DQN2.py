import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math, random
from environment_stage_4 import Env

import time
import rospy
from std_msgs.msg import Float32MultiArray
import os
import sys
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))

# -----------------------------
# è¶…å‚æ•°å®šä¹‰
# -----------------------------
BATCH_SIZE = 128
LR = 0.001
GAMMA = 0.9
TARGET_REPLACE_ITER = 10
MEMORY_CAPACITY = 2000
N_ACTIONS = 5
N_STATES = 28

# -----------------------------
# åˆå§‹åŒ–çŽ¯å¢ƒ
# -----------------------------
env = Env(N_ACTIONS)

# -----------------------------
# ç¥žç»ç½‘ç»œå®šä¹‰
# -----------------------------
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(N_STATES, 128)
        self.fc1.weight.data.normal_(0, 0.1)
        self.fc2 = nn.Linear(128, 128)
        self.fc2.weight.data.normal_(0, 0.1)
        self.out = nn.Linear(128, N_ACTIONS)
        self.out.weight.data.normal_(0, 0.1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        # æ³¨æ„ï¼šè¿™é‡Œä¸è¦å¯¹è¾“å‡ºåŠ  dropoutï¼Q å€¼éœ€è¦ç¨³å®š
        return self.out(x)

# -----------------------------
# DQN å®šä¹‰
# -----------------------------
class DQN(object):
    def __init__(self):
        self.eval_net, self.target_net = Net(), Net()
        self.learn_step_counter = 0
        self.memory_counter = 0
        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))  # æ›´æ¸…æ™°çš„ç»´åº¦
        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)
        self.loss_func = nn.MSELoss()
        self.start_epoch = 0
        self.epsilon = 1.0
        self.epsilon_min = 0.01
        self.load_models = True
        self.load_ep = 100
        self.loss = 0.0
        self.q_eval = 0.0
        self.q_target = 0.0

        if self.load_models:
            checkpoint = torch.load(f"./model/{self.load_ep}.pt")
            self.target_net.load_state_dict(checkpoint['target_net'])
            self.eval_net.load_state_dict(checkpoint['eval_net'])
            self.optimizer.load_state_dict(checkpoint['optimizer'])
            self.start_epoch = checkpoint['epoch'] + 1
            self.epsilon = 0.0  # åŠ è½½æ¨¡åž‹åŽ epsilon å¯ä»¥ä»Ž 0 å¼€å§‹ï¼Œæˆ–ç»§ç»­è¡°å‡
            print("âœ… Model loaded successfully.")

    def choose_action(self, x):
        x = torch.FloatTensor(x).unsqueeze(0)
        if np.random.uniform() > self.epsilon:
            actions_value = self.eval_net(x)
            action = actions_value.max(1)[1].item()
        else:
            action = np.random.randint(0, N_ACTIONS)
        return action

    def store_transition(self, s, a, r, s_):
        transition = np.hstack((s, [a, r], s_))
        index = self.memory_counter % MEMORY_CAPACITY
        self.memory[index, :] = transition
        self.memory_counter += 1

    def learn(self):
        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:
            self.target_net.load_state_dict(self.eval_net.state_dict())
        self.learn_step_counter += 1

        sample_index = np.random.choice(min(self.memory_counter, MEMORY_CAPACITY), BATCH_SIZE)
        b_memory = self.memory[sample_index]
        b_s = torch.FloatTensor(b_memory[:, :N_STATES])
        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int))
        b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])
        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])

        q_eval = self.eval_net(b_s).gather(1, b_a)
        q_next = self.target_net(b_s_).detach()
        q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)

        loss = self.loss_func(q_eval, q_target)
        self.loss = loss.item()
        self.q_eval = q_eval.mean().item()
        self.q_target = q_target.mean().item()

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

    def save_model(self, name):
        state = {
            'target_net': self.target_net.state_dict(),
            'eval_net': self.eval_net.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'epoch': self.start_epoch
        }
        torch.save(state, f"./model/{name}.pt")
        print(f"ðŸ’¾ Model saved: ./model/{name}.pt")

# -----------------------------
# âœ… æ•°æ®è®°å½•åˆ—è¡¨ï¼ˆè®­ç»ƒä¸­ä¿å­˜ï¼‰
# -----------------------------
rewards_log = []
losses_log = []
steps_log = []

# -----------------------------
# ä¸»ç¨‹åº
# -----------------------------
if __name__ == '__main__':
    dqn = DQN()
    rospy.init_node('turtlebot3_dqn_stage_4')
    pub_result = rospy.Publisher('result', Float32MultiArray, queue_size=5)
    result = Float32MultiArray()
    start_time = time.time()
    e = dqn.start_epoch

    # âœ… åˆ›å»º model ç›®å½•ï¼ˆé¿å…æŠ¥é”™ï¼‰
    os.makedirs("./model", exist_ok=True)

    for e in range(e, e + 10000):
        s = env.reset()
        rospy.loginfo(f"ðŸŽ¯ New goal at ({env.goal_x:.2f}, {env.goal_y:.2f})")
        episode_reward = 0
        done = False
        max_steps = 2500  # é˜²æ­¢å¡ä½ï¼

        for t in range(max_steps):
            a = dqn.choose_action(s)
            s_, r, done = env.step(a)
            dqn.store_transition(s, a, r, s_)
            episode_reward += r
            s = s_

            # å­¦ä¹ 
            if dqn.memory_counter > BATCH_SIZE:
                dqn.learn()

            # âœ… æ¯100æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡åž‹ï¼ˆå¯é€‰ï¼‰
            if t % 100 == 0 and e % 10 == 0:
                dqn.save_model(str(e))

            # âœ… è¶…æ—¶åˆ¤æ–­ï¼ˆé˜²æ­¢å¡ä½ï¼‰
            if t >= 2499:
                rospy.logwarn("â° Episode %d timed out at step %d", e, t)
                done = True

            # ç»“æŸ episode
            if done:
                break

            # epsilon è¡°å‡
            if dqn.epsilon > dqn.epsilon_min:
                dqn.epsilon -= 0.0001

        # --- âœ… è®°å½•æ•°æ® ---
        rewards_log.append(episode_reward)
        losses_log.append(dqn.loss)
        steps_log.append(t)

        # --- å‘å¸ƒç»“æžœ ---
        result.data = [episode_reward, dqn.loss, dqn.q_eval, dqn.q_target]
        pub_result.publish(result)

        # --- æ—¥å¿— ---
        m, s = divmod(int(time.time() - start_time), 60)
        h, m = divmod(m, 60)
        rospy.loginfo('Ep: %d | Score: %.2f | Steps: %d | Mem: %d | Eps: %.3f | Time: %d:%02d:%02d',
                      e, episode_reward, t, dqn.memory_counter, dqn.epsilon, h, m, s)

        # --- âœ… æ¯100è½®ä¿å­˜ä¸€æ¬¡æ•°æ® ---
        if e % 100 == 0:
            np.save("rewards_log.npy", np.array(rewards_log))
            np.save("losses_log.npy", np.array(losses_log))
            np.save("steps_log.npy", np.array(steps_log))
            dqn.save_model(str(e))
            rospy.loginfo("ðŸ“Š Data and model saved at episode %d", e)

    # --- âœ… è®­ç»ƒç»“æŸï¼Œä¿å­˜å…¨éƒ¨ ---
    np.save("rewards_log.npy", np.array(rewards_log))
    np.save("losses_log.npy", np.array(losses_log))
    np.save("steps_log.npy", np.array(steps_log))
    rospy.loginfo("ðŸŽ‰ Training finished. All logs saved.")

# import torch                                    # å¯¼å…¥torch
# import torch.nn as nn                           # å¯¼å…¥torch.nn
# import torch.nn.functional as F                 # å¯¼å…¥torch.nn.functional
# import numpy as np                              # å¯¼å…¥numpy
# # import gym                                      # å¯¼å…¥gym
# import math, random
# from environment_stage_4 import Env

# import time
# import rospy
# from std_msgs.msg import Float32MultiArray
# import os
# import sys
# sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))
# # è¶…å‚æ•°
# BATCH_SIZE = 128                               # æ ·æœ¬æ•°é‡
# LR = 0.001                                       # å­¦ä¹ çŽ‡
# EPISODES=0.99                          # greedy policy
# GAMMA = 0.9                                     # reward discount
# TARGET_REPLACE_ITER = 10                       # ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘çŽ‡
# MEMORY_CAPACITY = 2000                          # è®°å¿†åº“å®¹é‡
# N_ACTIONS = 5
# env =Env(N_ACTIONS)        # ä½¿ç”¨gymåº“ä¸­çš„çŽ¯å¢ƒï¼šCartPoleï¼Œä¸”æ‰“å¼€å°è£…(è‹¥æƒ³äº†è§£è¯¥çŽ¯å¢ƒï¼Œè¯·è‡ªè¡Œç™¾åº¦)             # æ†å­åŠ¨ä½œä¸ªæ•° (2ä¸ª)
# N_STATES =  28  # æ†å­çŠ¶æ€ä¸ªæ•° (4ä¸ª)


# """
# torch.nnæ˜¯ä¸“é—¨ä¸ºç¥žç»ç½‘ç»œè®¾è®¡çš„æ¨¡å—åŒ–æŽ¥å£ã€‚nnæž„å»ºäºŽAutogradä¹‹ä¸Šï¼Œå¯ä»¥ç”¨æ¥å®šä¹‰å’Œè¿è¡Œç¥žç»ç½‘ç»œã€‚
# nn.Moduleæ˜¯nnä¸­ååˆ†é‡è¦çš„ç±»ï¼ŒåŒ…å«ç½‘ç»œå„å±‚çš„å®šä¹‰åŠforwardæ–¹æ³•ã€‚
# å®šä¹‰ç½‘ç»œï¼š
#     éœ€è¦ç»§æ‰¿nn.Moduleç±»ï¼Œå¹¶å®žçŽ°forwardæ–¹æ³•ã€‚
#     ä¸€èˆ¬æŠŠç½‘ç»œä¸­å…·æœ‰å¯å­¦ä¹ å‚æ•°çš„å±‚æ”¾åœ¨æž„é€ å‡½æ•°__init__()ä¸­ã€‚
#     åªè¦åœ¨nn.Moduleçš„å­ç±»ä¸­å®šä¹‰äº†forwardå‡½æ•°ï¼Œbackwardå‡½æ•°å°±ä¼šè¢«è‡ªåŠ¨å®žçŽ°(åˆ©ç”¨Autograd)ã€‚
# """


# # å®šä¹‰Netç±» (å®šä¹‰ç½‘ç»œ)
# class Net(nn.Module):
#     def __init__(self):                                                         # å®šä¹‰Netçš„ä¸€ç³»åˆ—å±žæ€§
#         # nn.Moduleçš„å­ç±»å‡½æ•°å¿…é¡»åœ¨æž„é€ å‡½æ•°ä¸­æ‰§è¡Œçˆ¶ç±»çš„æž„é€ å‡½æ•°
#         super(Net, self).__init__()                                             # ç­‰ä»·ä¸Žnn.Module.__init__()

#         self.fc1 = nn.Linear(N_STATES, 128)                                      # è®¾ç½®ç¬¬ä¸€ä¸ªå…¨è¿žæŽ¥å±‚(è¾“å…¥å±‚åˆ°éšè—å±‚): çŠ¶æ€æ•°ä¸ªç¥žç»å…ƒåˆ°50ä¸ªç¥žç»å…ƒ
#         self.fc1.weight.data.normal_(0, 0.1)                                    # æƒé‡åˆå§‹åŒ– (å‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º0.1çš„æ­£æ€åˆ†å¸ƒ)
#         self.fc2 =nn.Linear(128,128)
#         self.fc2.weight.data.normal_(0,0.1)
#         self.out = nn.Linear(128, N_ACTIONS)                                     # è®¾ç½®ç¬¬äºŒä¸ªå…¨è¿žæŽ¥å±‚(éšè—å±‚åˆ°è¾“å‡ºå±‚): 50ä¸ªç¥žç»å…ƒåˆ°åŠ¨ä½œæ•°ä¸ªç¥žç»å…ƒ
#         self.out.weight.data.normal_(0, 0.1)                                    # æƒé‡åˆå§‹åŒ– (å‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º0.1çš„æ­£æ€åˆ†å¸ƒ)

#     def forward(self, x):                                                       # å®šä¹‰forwardå‡½æ•° (xä¸ºçŠ¶æ€)
#         x = F.relu(self.fc1(x))                                                 # è¿žæŽ¥è¾“å…¥å±‚åˆ°éšè—å±‚ï¼Œä¸”ä½¿ç”¨æ¿€åŠ±å‡½æ•°ReLUæ¥å¤„ç†ç»è¿‡éšè—å±‚åŽçš„å€¼
#         x=F.relu(self.fc2(x))
#         x=F.dropout(self.fc2(x))
#         actions_value = self.out(x)                                             # è¿žæŽ¥éšè—å±‚åˆ°è¾“å‡ºå±‚ï¼ŒèŽ·å¾—æœ€ç»ˆçš„è¾“å‡ºå€¼ (å³åŠ¨ä½œå€¼)
#         return actions_value                                                    # è¿”å›žåŠ¨ä½œå€¼


# # å®šä¹‰DQNç±» (å®šä¹‰ä¸¤ä¸ªç½‘ç»œ)
# class DQN(object):
#     def __init__(self):                                                         # å®šä¹‰DQNçš„ä¸€ç³»åˆ—å±žæ€§
#         self.eval_net, self.target_net = Net(), Net()                           # åˆ©ç”¨Netåˆ›å»ºä¸¤ä¸ªç¥žç»ç½‘ç»œ: è¯„ä¼°ç½‘ç»œå’Œç›®æ ‡ç½‘ç»œ
#         self.learn_step_counter = 0                                             # for target updating
#         self.memory_counter = 0                                                 # for storing memory
#         self.memory = np.zeros((MEMORY_CAPACITY, 58))             # åˆå§‹åŒ–è®°å¿†åº“ï¼Œä¸€è¡Œä»£è¡¨ä¸€ä¸ªtransition
#         self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)    # ä½¿ç”¨Adamä¼˜åŒ–å™¨ (è¾“å…¥ä¸ºè¯„ä¼°ç½‘ç»œçš„å‚æ•°å’Œå­¦ä¹ çŽ‡)
#         self.loss_func = nn.MSELoss()                                           # ä½¿ç”¨å‡æ–¹æŸå¤±å‡½æ•° (loss(xi, yi)=(xi-yi)^2)
#         self.start_epoch = 0
#         self.epsilon = 1.0
#         self.epsilon_min = 0.01
#         self.load_models=True
#         self.load_ep =200
#         self.loss =0
#         self.q_eval=0
#         self.q_target=0
#         if self.load_models:
#             self.epsilon= 0
#             self.start_epoch=self.load_ep
#             checkpoint = torch.load("./model/"+str(self.load_ep)+".pt")
#             print(checkpoint.keys())
#             print(checkpoint['epoch'])
#             print(checkpoint)
#             self.target_net.load_state_dict(checkpoint['target_net'])
#             self.eval_net.load_state_dict(checkpoint['eval_net'])
#             self.optimizer.load_state_dict(checkpoint['optimizer'])
#             self.start_epoch = checkpoint['epoch'] + 1
#             print("loadmodel")
#     def choose_action(self, x):                                                 # å®šä¹‰åŠ¨ä½œé€‰æ‹©å‡½æ•° (xä¸ºçŠ¶æ€)
#         x = torch.unsqueeze(torch.FloatTensor(x), 0)                            # å°†xè½¬æ¢æˆ32-bit floating pointå½¢å¼ï¼Œå¹¶åœ¨dim=0å¢žåŠ ç»´æ•°ä¸º1çš„ç»´åº¦
#         if np.random.uniform() > self.epsilon:                                       # ç”Ÿæˆä¸€ä¸ªåœ¨[0, 1)å†…çš„éšæœºæ•°ï¼Œå¦‚æžœå°äºŽEPSILONï¼Œé€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
#             actions_value = self.eval_net.forward(x)                            # é€šè¿‡å¯¹è¯„ä¼°ç½‘ç»œè¾“å…¥çŠ¶æ€xï¼Œå‰å‘ä¼ æ’­èŽ·å¾—åŠ¨ä½œå€¼
#             action = torch.max(actions_value, 1)[1].data.numpy()                # è¾“å‡ºæ¯ä¸€è¡Œæœ€å¤§å€¼çš„ç´¢å¼•ï¼Œå¹¶è½¬åŒ–ä¸ºnumpy ndarrayå½¢å¼

#             action = action[0]                                                  # è¾“å‡ºactionçš„ç¬¬ä¸€ä¸ªæ•°
#         else:                                                                   # éšæœºé€‰æ‹©åŠ¨ä½œ
#             action = np.random.randint(0, N_ACTIONS)                            # è¿™é‡Œactionéšæœºç­‰äºŽ0æˆ–1 (N_ACTIONS = 2)
#         return action                                                           # è¿”å›žé€‰æ‹©çš„åŠ¨ä½œ (0æˆ–1)

#     def store_transition(self, s, a, r, s_):                                    # å®šä¹‰è®°å¿†å­˜å‚¨å‡½æ•° (è¿™é‡Œè¾“å…¥ä¸ºä¸€ä¸ªtransition)
#         transition = np.hstack((s, [a, r], s_))                                 # åœ¨æ°´å¹³æ–¹å‘ä¸Šæ‹¼æŽ¥æ•°ç»„
#         # å¦‚æžœè®°å¿†åº“æ»¡äº†ï¼Œä¾¿è¦†ç›–æ—§çš„æ•°æ®
#         index = self.memory_counter % MEMORY_CAPACITY                           # èŽ·å–transitionè¦ç½®å…¥çš„è¡Œæ•°
#         self.memory[index, :] = transition                                      # ç½®å…¥transition
#         self.memory_counter += 1                                                # memory_counterè‡ªåŠ 1

#     def learn(self):                                                            # å®šä¹‰å­¦ä¹ å‡½æ•°(è®°å¿†åº“å·²æ»¡åŽä¾¿å¼€å§‹å­¦ä¹ )
#         # ç›®æ ‡ç½‘ç»œå‚æ•°æ›´æ–°
#         if self.learn_step_counter % TARGET_REPLACE_ITER == 0:                  # ä¸€å¼€å§‹è§¦å‘ï¼Œç„¶åŽæ¯100æ­¥è§¦å‘
#             self.target_net.load_state_dict(self.eval_net.state_dict())         # å°†è¯„ä¼°ç½‘ç»œçš„å‚æ•°èµ‹ç»™ç›®æ ‡ç½‘ç»œ
#         self.learn_step_counter += 1                                            # å­¦ä¹ æ­¥æ•°è‡ªåŠ 1

#         # æŠ½å–è®°å¿†åº“ä¸­çš„æ‰¹æ•°æ®
#         sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)            # åœ¨[0, 2000)å†…éšæœºæŠ½å–32ä¸ªæ•°ï¼Œå¯èƒ½ä¼šé‡å¤
#         b_memory = self.memory[sample_index, :]                                 # æŠ½å–32ä¸ªç´¢å¼•å¯¹åº”çš„32ä¸ªtransitionï¼Œå­˜å…¥b_memory
#         b_s = torch.FloatTensor(b_memory[:, :N_STATES])
#         # å°†32ä¸ªsæŠ½å‡ºï¼Œè½¬ä¸º32-bit floating pointå½¢å¼ï¼Œå¹¶å­˜å‚¨åˆ°b_sä¸­ï¼Œb_sä¸º32è¡Œ4åˆ—
#         b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int))
#         # å°†32ä¸ªaæŠ½å‡ºï¼Œè½¬ä¸º64-bit integer (signed)å½¢å¼ï¼Œå¹¶å­˜å‚¨åˆ°b_aä¸­ (ä¹‹æ‰€ä»¥ä¸ºLongTensorç±»åž‹ï¼Œæ˜¯ä¸ºäº†æ–¹ä¾¿åŽé¢torch.gatherçš„ä½¿ç”¨)ï¼Œb_aä¸º32è¡Œ1åˆ—
#         b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])
#         # å°†32ä¸ªræŠ½å‡ºï¼Œè½¬ä¸º32-bit floating pointå½¢å¼ï¼Œå¹¶å­˜å‚¨åˆ°b_sä¸­ï¼Œb_rä¸º32è¡Œ1åˆ—
#         b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])
#         # å°†32ä¸ªs_æŠ½å‡ºï¼Œè½¬ä¸º32-bit floating pointå½¢å¼ï¼Œå¹¶å­˜å‚¨åˆ°b_sä¸­ï¼Œb_s_ä¸º32è¡Œ4åˆ—

#         # èŽ·å–32ä¸ªtransitionçš„è¯„ä¼°å€¼å’Œç›®æ ‡å€¼ï¼Œå¹¶åˆ©ç”¨æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨è¿›è¡Œè¯„ä¼°ç½‘ç»œå‚æ•°æ›´æ–°
#         q_eval = self.eval_net(b_s).gather(1, b_a)
#         # eval_net(b_s)é€šè¿‡è¯„ä¼°ç½‘ç»œè¾“å‡º32è¡Œæ¯ä¸ªb_så¯¹åº”çš„ä¸€ç³»åˆ—åŠ¨ä½œå€¼ï¼Œç„¶åŽ.gather(1, b_a)ä»£è¡¨å¯¹æ¯è¡Œå¯¹åº”ç´¢å¼•b_açš„Qå€¼æå–è¿›è¡Œèšåˆ
#         q_next = self.target_net(b_s_).detach()
#         # q_nextä¸è¿›è¡Œåå‘ä¼ é€’è¯¯å·®ï¼Œæ‰€ä»¥detachï¼›q_nextè¡¨ç¤ºé€šè¿‡ç›®æ ‡ç½‘ç»œè¾“å‡º32è¡Œæ¯ä¸ªb_s_å¯¹åº”çš„ä¸€ç³»åˆ—åŠ¨ä½œå€¼
#         q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)
#         # q_next.max(1)[0]è¡¨ç¤ºåªè¿”å›žæ¯ä¸€è¡Œçš„æœ€å¤§å€¼ï¼Œä¸è¿”å›žç´¢å¼•(é•¿åº¦ä¸º32çš„ä¸€ç»´å¼ é‡)ï¼›.view()è¡¨ç¤ºæŠŠå‰é¢æ‰€å¾—åˆ°çš„ä¸€ç»´å¼ é‡å˜æˆ(BATCH_SIZE, 1)çš„å½¢çŠ¶ï¼›æœ€ç»ˆé€šè¿‡å…¬å¼å¾—åˆ°ç›®æ ‡å€¼
#         loss = self.loss_func(q_eval, q_target)
#         self.loss = torch.max(loss)
#         self.q_eval =torch.max(q_eval)
#         self.q_target =torch.max(q_target)
#         # è¾“å…¥32ä¸ªè¯„ä¼°å€¼å’Œ32ä¸ªç›®æ ‡å€¼ï¼Œä½¿ç”¨å‡æ–¹æŸå¤±å‡½æ•°
#         self.optimizer.zero_grad()                                      # æ¸…ç©ºä¸Šä¸€æ­¥çš„æ®‹ä½™æ›´æ–°å‚æ•°å€¼
#         loss.backward()                                                 # è¯¯å·®åå‘ä¼ æ’­, è®¡ç®—å‚æ•°æ›´æ–°å€¼
#         self.optimizer.step()                                           # æ›´æ–°è¯„ä¼°ç½‘ç»œçš„æ‰€æœ‰å‚æ•°



        

#     def save_model(self,dir):
#         state = {'target_net':self.target_net.state_dict(),'eval_net':self.eval_net.state_dict(), 'optimizer':self.optimizer.state_dict(), 'epoch':e}
#         torch.save(state,"./model/"+ dir+".pt")

# if __name__=='__main__':

#     dqn = DQN()                                                             # ä»¤dqn=DQNç±»
#     rospy.init_node('turtlebot3_dqn_stage_4')
#     pub_result = rospy.Publisher('result', Float32MultiArray, queue_size=5)
#     pub_get_action = rospy.Publisher('get_action', Float32MultiArray, queue_size=5)
#     result = Float32MultiArray()
#     get_action = Float32MultiArray()
#     start_time =time.time()
#     e=dqn.start_epoch
# for e in range(10000):                                                    # 400ä¸ªepisodeå¾ªçŽ¯
#     s = env.reset()
#     rospy.loginfo(f"New goal at ({env.goal_x}, {env.goal_y})")                                                   # é‡ç½®çŽ¯å¢ƒ
#     episode_reward_sum = 0                                              # åˆå§‹åŒ–è¯¥å¾ªçŽ¯å¯¹åº”çš„episodeçš„æ€»å¥–åŠ±
#     done=False
#     episode_step=6000

#     for t in range(episode_step):                                                         # å¼€å§‹ä¸€ä¸ªepisode (æ¯ä¸€ä¸ªå¾ªçŽ¯ä»£è¡¨ä¸€æ­¥)
#         a = dqn.choose_action(s)                                        # è¾“å…¥è¯¥æ­¥å¯¹åº”çš„çŠ¶æ€sï¼Œé€‰æ‹©åŠ¨ä½œ
#         s_, r, done = env.step(a)                                 # æ‰§è¡ŒåŠ¨ä½œï¼ŒèŽ·å¾—åé¦ˆ

#         # # ä¿®æ”¹å¥–åŠ± (ä¸ä¿®æ”¹ä¹Ÿå¯ä»¥ï¼Œä¿®æ”¹å¥–åŠ±åªæ˜¯ä¸ºäº†æ›´å¿«åœ°å¾—åˆ°è®­ç»ƒå¥½çš„æ‘†æ†)
#         # x, x_dot, theta, theta_dot = s_
#         # r1 = (env.x_threshold - abs(x)) / env.x_threshold - 0.8
#         # r2 = (env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians - 0.5
#         # new_r = r1 + r2

#         dqn.store_transition(s, a, r, s_)                 # å­˜å‚¨æ ·æœ¬
#         episode_reward_sum += r                           # é€æ­¥åŠ ä¸Šä¸€ä¸ªepisodeå†…æ¯ä¸ªstepçš„reward
#         s = s_                                                # æ›´æ–°çŠ¶æ€
#         pub_get_action.publish(get_action)
#         if dqn.memory_counter > BATCH_SIZE:              # å¦‚æžœç´¯è®¡çš„transitionæ•°é‡è¶…è¿‡äº†è®°å¿†åº“çš„å›ºå®šå®¹é‡2000
#             # å¼€å§‹å­¦ä¹  (æŠ½å–è®°å¿†ï¼Œå³32ä¸ªtransitionï¼Œå¹¶å¯¹è¯„ä¼°ç½‘ç»œå‚æ•°è¿›è¡Œæ›´æ–°ï¼Œå¹¶åœ¨å¼€å§‹å­¦ä¹ åŽæ¯éš”100æ¬¡å°†è¯„ä¼°ç½‘ç»œçš„å‚æ•°èµ‹ç»™ç›®æ ‡ç½‘ç»œ)
#             dqn.learn()
#         if e % 10 ==0:
#             dqn.save_model(str(e))
#         if t >=2500:
#             rospy.loginfo("time out!")
#             done =True


#         if done:       # å¦‚æžœdoneä¸ºTrue
#             # round()æ–¹æ³•è¿”å›žepisode_reward_sumçš„å°æ•°ç‚¹å››èˆäº”å…¥åˆ°2ä¸ªæ•°å­—
#             result.data =[episode_reward_sum,float(dqn.loss),float(dqn.q_eval),float(dqn.q_target)]
#             pub_result.publish(result)
#             m,s =divmod(int(time.time()- start_time),60)
#             h,m =divmod(m,60)
#             rospy.loginfo('Ep: %d score: %.2f memory: %d epsilon: %.2f time: %d:%02d:%02d',e, episode_reward_sum, dqn.memory_counter, dqn.epsilon, h, m, s)
#             # print('episode%s---reward_sum: %s' % (i, round(episode_reward_sum, 2)))
#             param_keys = ['epsilon']
#             param_values = [dqn.epsilon]
#             param_dictionary = dict(zip(param_keys, param_values))

#             break                                             # è¯¥episodeç»“æŸ
#         if dqn.epsilon > dqn.epsilon_min :
#             dqn.epsilon =dqn.epsilon-0.0001